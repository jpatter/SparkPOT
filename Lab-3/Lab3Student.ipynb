{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Apache Spark lab, Lab 3: Binary Classification with Spark ML\n",
    "\n",
    "### In this notebook, we will explore Binary Classification using Spark ML. We will exploit Spark ML's high-level APIs built on top of DataFrames to create and tune machine learning pipelines. Spark ML Pipelines enable combining multiple algorithms into a single pipeline or workflow. We will heavily utilize Spark ML's feature transformers to convert, modify and scale the features that will be used to develop the machine learning model. Finally, we will evaluate and cross validate our model to demonstrate the process of determining a best fit model.\n",
    "\n",
    "### The binary classification demo will utilize the famous Titanic dataset, which has been used for Kaggle competitions and can be downloaded here. There is no need to download the data manually as it is downloaded directly within the noteboook.\n",
    "https://www.kaggle.com/c/titanic/data\n",
    "\n",
    "\n",
    "### The Titanic data set was chosen for this binary classification demonstration because it contains both text based and numeric features that are both continuous and categorical. This will give us the opportunity to explore and utilize a number of feature transformers available in Spark ML.\n",
    "     \n",
    "          \n",
    "\n",
    "![IBM Logo](https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcSzlUYaJ9xykGC-N5PijcV_eDBGCXy_pMn7sy6ymrVypmJ22q5ZmA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of contents\n",
    "\n",
    "1. [Install needed libraries](#libraries)<br/>\n",
    "2. [Get the Data](#getdata)<br/>\n",
    "3. [Prepare and clean the data](#prepare)<br/>\n",
    "    3.1 [Remove unneeded columns](#remove)<br/>\n",
    "4. [Transform the data](#transform)<br/>\n",
    "    4.1 [Gender and Embarkation](#stringindexer)<br/>\n",
    "    4.2 [Age and Fare](#bucketizer)<br/>\n",
    "5. [Build the Model](#build)<br/>\n",
    "6. [Split the data into train and test sets](#split)<br/>\n",
    "7. [Test the Model](#test)<br/>\n",
    "8. [Tune the Model](#tune)<br/>\n",
    "9. [Predict imaginary passenger](#predict)<br/>\n",
    "10. [Random Forest](#randomforest)<br/>\n",
    "11. [Summary](#summary)<br/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Verify Spark version and existence of Spark and Spark SQL contexts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The spark version is {}.'.format(spark.version))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"libraries\"></a>\n",
    "## 1 - Import required Spark libraries \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After executing this block, you should see a message saying that the `Pixiedust database opened successfully`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "from pyspark.ml.feature import Bucketizer\n",
    "from pyspark.mllib.linalg import Vectors\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import Normalizer\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "#import pixiedust display module\n",
    "from pixiedust.display import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"getdata\"></a>\n",
    "## 2 - Download Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -f Titanic.csv\n",
    "!wget https://ibm.box.com/shared/static/crceca9g1ym3nl0hwaxa5c0j0m3e19l8.csv -O Titanic.csv -q\n",
    "!ls -l Titanic.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data in as a DataFrame\n",
    "### Source data is in CSV format and includes a header. We will ask Spark to infer the schema/data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loadTitanicData = sqlContext.read.format(\"org.apache.spark.sql.execution.datasources.csv.CSVFileFormat\").option(\"header\", \"true\").option(\"inferSchema\", \"true\").load(\"Titanic.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prepare\"></a>\n",
    "## 3. Prepare and shape the data\n",
    "\n",
    "PixieDust is an open-source IBM library which can be used to easily and flexibly `display` data.\n",
    "\n",
    "Use PixieDust to examine the schema (click on the Schema line).   Try differing displays of the data using PixieDust.\n",
    "\n",
    "For example, try showing a histogram of `fare` or `age` or `pclass`.    Change the renderer and see what happens.\n",
    "<br>\n",
    " <div class=\"panel-group\" id=\"accordion-3\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-3\" href=\"#collapse1-3\">\n",
    "        Hint 1</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-3\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Select the Chart icon and select Histogram</div>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-3\" href=\"#collapse2-3\">\n",
    "        Hint 2</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse2-3\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Select the Options button.   Drag the age (or fare or class) field to the values column.   Change number of rows to display to more than the number of rows read in (1400 will do)</div>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-3\" href=\"#collapse3-3\">\n",
    "        Hint 3</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse3-3\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">Change the renderer (dropdown on upper right) to something else, such as `seaborn` or `bokeh`</div>\n",
    "    </div>\n",
    "  </div>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pixiedust": {
     "displayParams": {
      "handlerId": "histogram",
      "rendererId": "matplotlib",
      "rowCount": "500",
      "valueFields": "Age"
     }
    }
   },
   "outputs": [],
   "source": [
    "display(loadTitanicData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"remove\"></a>\n",
    "## 3.1 - Drop unwanted columns and rows with null or invalid data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loadTitanicData = loadTitanicData.drop(\"PassengerId\").drop(\"Name\").drop(\"Ticket\").drop(\"Cabin\").dropna(how=\"any\", subset=(\"Age\", \"Embarked\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  We will use the 'Survived' column as a label for training the machine learning model\n",
    "#### Spark ML requires that that the labels are data type Double, so we will cast the  column as Double (it was inferred as Integer when read into Spark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "LabeledTitanicData = (loadTitanicData.withColumn(\"SurvivedTemp\", loadTitanicData[\"Survived\"]\n",
    "    .cast(\"Double\")).drop(\"Survived\")\n",
    "    .withColumnRenamed(\"SurvivedTemp\", \"Survived\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LabeledTitanicData.sample(False, 0.01, seed=0).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print some record counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The total number of rows is {}.'.format(LabeledTitanicData.count()))\n",
    "print('The number of rows labeled Not Survived is {}.'.format(LabeledTitanicData.filter(LabeledTitanicData['Survived'] == 0).count()))\n",
    "print('The number of rows labeled Survived is {}.'.format(LabeledTitanicData.filter(LabeledTitanicData['Survived'] == 1).count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show the schema of the labeled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LabeledTitanicData.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"transform\"></a>\n",
    "## 4. Transform the data\n",
    "\n",
    "Certain data fields need to be transformed before building the model.   This can be for several reasons ranging from needing to convert String values to numeric values or shaping data into different formats.\n",
    "\n",
    "<a id=\"stringindexer\"></a>\n",
    "## 4.1 Use <a href=\"https://spark.apache.org/docs/latest/ml-features.html#stringindexer\">StringIndexer</a> to transform gender and embarked values\n",
    "\n",
    "StringIndexer is a transformer that encodes a string column to a column of indices. The indices are ordered by value frequencies, so the most frequent value gets index 0. If the input column is numeric, it is cast to string first. \n",
    "\n",
    "For the Titanic data set, we will index the Sex/Gender column as well as the Embarked column, which specifies at which  port the passenger boarded the ship.## StringIndexer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "SexIndexer = StringIndexer(inputCol=\"Sex\", outputCol=\"SexIndex\")\n",
    "EmbarkedIndexer = StringIndexer(inputCol=\"Embarked\", outputCol=\"EmbarkedIndex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bucketizer\"></a>\n",
    "## 4.2 <a href=\"https://spark.apache.org/docs/latest/ml-features.html#bucketizer\">Bucketizer</a> is a transformer that transforms a column of continuous features to a column of feature buckets, where the buckets are by a splits parameter. \n",
    "\n",
    "For the Titanic data set, we will index the Age and Fare features.\n",
    "\n",
    "<br/>\n",
    "<div class=\"panel-group\" id=\"accordion-42\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-42\" href=\"#collapse1-42\">\n",
    "        Advanced Optional</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-42\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">After completing the lab, note the prediction percentage then come back and change the values for either Bucketizer and re-run the kernel [Kernel->Restart and Run All].   Note the change in prediction accuracy.</div>\n",
    "    </div>\n",
    "  </div>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AgeBucketSplits = [0.0, 6.0, 12.0, 18.0, 40.0, 65.0, 80.0, float(\"inf\")]\n",
    "AgeBucket = Bucketizer(splits=AgeBucketSplits, inputCol=\"Age\", outputCol=\"AgeBucket\")\n",
    "\n",
    "FareBucketSplits = [0.0, 10.0, 20.0, 30.0, 40.0, 50.0, 80.0, 100.0, float(\"inf\")]\n",
    "FareBucket = Bucketizer(splits=FareBucketSplits, inputCol=\"Fare\", outputCol=\"FareBucket\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"build\"></a>\n",
    "## 5. Building the Model\n",
    "\n",
    "## <a href=\"https://spark.apache.org/docs/latest/ml-features.html#vectorassembler\">VectorAssembler</a> is a transformer that combines a given list of columns in the order specified into a single vector column in order to train a model.\n",
    "\n",
    "<br/>\n",
    "<div class=\"panel-group\" id=\"accordion-5\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-5\" href=\"#collapse1-5\">\n",
    "        Advanced Optional</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-5\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">After completing the lab, note the prediction percentage then come back and remove some of the values in the assembler (i.e. remove sibsp, pclass and parch or remove SexIndex) and re-run the kernel [Kernel->Restart and Run All].   Note the change in prediction accuracy.</div>\n",
    "    </div>\n",
    "  </div>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols= [\"SexIndex\", \"EmbarkedIndex\", \"AgeBucket\", \"FareBucket\", \"SibSp\", \"Pclass\", \"Parch\"], outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizer is a Transformer which transforms a dataset of Vector rows, normalizing each Vector to have unit norm\n",
    "### This normalization can help standardize your input data and improve the behavior of learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normalizer = Normalizer(inputCol=\"features\", outputCol=\"normFeatures\", p=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression is a popular method to predict a binary response\n",
    "### It is a special case of Generalized Linear models that predicts the probability of an outcome."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"normFeatures\", labelCol=\"Survived\", predictionCol=\"prediction\", maxIter=10, regParam=0.1, elasticNetParam=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Pipeline is a sequence of stages where each stage is either a Transformer or an Estimator\n",
    "### These stages are run in order and the input DataFrame is transformed as it passes through each stage. \n",
    "\n",
    "### In machine learning, it is common to run a sequence of algorithms to process and learn from data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[SexIndexer, EmbarkedIndexer, AgeBucket,FareBucket, assembler, normalizer, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"split\"></a>\n",
    "## 6 - Split the data into training (90%) and testing (10%) sets using <a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame.randomSplit\">random_split()</a>\n",
    "\n",
    "Set seed to 1 in order to make certain this is repeatable.\n",
    "<br>\n",
    " <div class=\"panel-group\" id=\"accordion-6\">\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-6\" href=\"#collapse1-6\">\n",
    "        Hint 1</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse1-6\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">train, test = LabeledTitanicData.randomSplit()</div>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-6\" href=\"#collapse2-6\">\n",
    "        Hint 2</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse2-6\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">train, test = LabeledTitanicData.randomSplit([??,??], seed=??)</div>\n",
    "    </div>\n",
    "  </div>\n",
    "  <div class=\"panel panel-default\">\n",
    "    <div class=\"panel-heading\">\n",
    "      <h4 class=\"panel-title\">\n",
    "        <a data-toggle=\"collapse\" data-parent=\"#accordion-6\" href=\"#collapse3-6\">\n",
    "        Solution</a>\n",
    "      </h4>\n",
    "    </div>\n",
    "    <div id=\"collapse3-6\" class=\"panel-collapse collapse\">\n",
    "      <div class=\"panel-body\">train, test = LabeledTitanicData.randomSplit([90.0,10.0], seed=1)</div>\n",
    "    </div>\n",
    "  </div>\n",
    "</div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test = LabeledTitanicData.\n",
    "train.cache()\n",
    "test.cache()\n",
    "print('The number of records in the traininig data set is {}.'.format(train.count()))\n",
    "print('The number of rows labeled Not Survived in the training data set is {}.'.format(train.filter(train['Survived'] == 0).count()))\n",
    "print('The number of rows labeled Survived in the training data set is {}.'.format(train.filter(train['Survived'] == 1).count()))\n",
    "train.sample(False, 0.01, seed=0).show(5)\n",
    "print('')\n",
    "\n",
    "print('The number of records in the test data set is {}.'.format(test.count()))\n",
    "print('The number of rows labeled Not Survived in the test data set is {}.'.format(test.filter(train['Survived'] == 0).count()))\n",
    "print('The number of rows labeled Survived in the test data set is {}.'.format(test.filter(train['Survived'] == 1).count()))\n",
    "test.sample(False, 0.1, seed=0).show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit the pipeline to the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = pipeline.fit(train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"test\"></a>\n",
    "## 7 - Make predictions on passengers in the Test data set\n",
    "### Keep in mind that the model has not seen the data in the test data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "predictions = model.transform(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.sample(False, 0.1, seed=0).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The number of predictions labeled Not Survived is {}.'.format(predictions.filter(predictions['prediction'] == 0).count()))\n",
    "print('The number of predictions labeled Survived is {}.'.format(predictions.filter(predictions['prediction'] == 1).count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(predictions.filter(\"Survived = 0.0\")\n",
    "     .select(\"Sex\", \"Age\", \"Fare\", \"Embarked\", \"Pclass\", \"Parch\", \"SibSp\", \"Survived\", \"prediction\")\n",
    "     .sample(False, 0.1, seed=0).show(5))\n",
    "\n",
    "(predictions.filter(\"Survived = 1.0\")\n",
    "     .select(\"Sex\", \"Age\", \"Fare\", \"Embarked\", \"Pclass\", \"Parch\", \"SibSp\", \"Survived\", \"prediction\")\n",
    "     .sample(False, 0.5, seed=0).show(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Create an evaluator for the binary classification using area under the ROC Curve as the evaluation metric\n",
    "\n",
    "### Receiver operating characteristic (ROC) is a graphical plot that illustrates the performance of a binary classifier system as its discrimination threshold is varied\n",
    "\n",
    "The curve is created by plotting the true positive rate against the false positive rate at various threshold settings. The ROC curve is thus the sensitivity as a function of fall-out. The area under the ROC curve is useful for comparing and selecting the best machine learning model for a given data set. A model with an area under the ROC curve score near 1 has very good performance. A model with a score near 0.5 is about as good as flipping a coin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = BinaryClassificationEvaluator().setLabelCol(\"Survived\").setMetricName(\"areaUnderROC\")\n",
    "print('Area under the ROC curve = {}.'.format(evaluator.evaluate(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"tune\"></a>\n",
    "## 8 - Tune Hyperparameters\n",
    "### Generate hyperparameter combinations by taking the cross product of some parameter values\n",
    "\n",
    "Spark ML algorithms provide many hyperparameters for tuning models. These hyperparameters are distinct from the model parameters being optimized by Spark ML itself. Hyperparameter tuning is accomplished by choosing the best set of parameters based on model performance on test data that the model was not trained with. All combinations of hyperparameters specified will be tried in order to find the one that leads to the model with the best evaluation result."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a Parameter Grid specifying what parameters and values will be evaluated in order to determine the best combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paramGrid = (ParamGridBuilder().addGrid(lr.regParam, [0.0, 0.1, 0.3])\n",
    "                 .addGrid(lr.elasticNetParam, [0.0, 0.8, 1.0])\n",
    "                 .addGrid(normalizer.p, [1.0, 2.0])\n",
    "                 .build())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a cross validator to tune the pipeline with the generated parameter grid\n",
    "Spark ML provides for cross-validation for hyperparameter tuning. Cross-validation attempts to fit the underlying estimator with user-specified combinations of parameters, cross-evaluate the fitted models, and output the best one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cv = CrossValidator().setEstimator(pipeline).setEvaluator(evaluator).setEstimatorParamMaps(paramGrid).setNumFolds(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-evaluate the ML Pipeline to find the best model\n",
    "### using the area under the ROC evaluator and hyperparameters specified in the parameter grid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cvModel = cv.fit(LabeledTitanicData)\n",
    "print('Area under the ROC curve for best fitted model = {}.'.format(evaluator.evaluate(cvModel.transform(LabeledTitanicData))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's see what improvement we achieve by tuning the hyperparameters using cross-evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Area under the ROC curve for non-tuned model = {}.'.format(evaluator.evaluate(predictions)))\n",
    "print('Area under the ROC curve for best fitted model = {}.'.format(evaluator.evaluate(cvModel.transform(LabeledTitanicData))))\n",
    "print('Improvement = {0:0.2f}%'.format((evaluator.evaluate(cvModel.transform(LabeledTitanicData)) - evaluator.evaluate(predictions)) *100 / evaluator.evaluate(predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make improved predictions using the Cross-validated model\n",
    "### Using the Test data set and DataFrame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cvModel.transform(test).select(\"Survived\", \"prediction\").sample(False, 0.1, seed=0).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Like above, but now using SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create temporary table\n",
    "cvModel.transform(test).createOrReplaceTempView(\"cvModelPredictions\")\n",
    "spark.sql(\"select Survived, prediction from cvModelPredictions\").sample(False, 0.1, seed=0).show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"predict\"></a>\n",
    "## 9 - Make a prediction on an imaginary passenger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the imaginary passenger's features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SexValue = 'female'\n",
    "AgeValue = 40.0\n",
    "FareValue = 15.0\n",
    "EmbarkedValue = 'C'\n",
    "PclassValue = 2\n",
    "SibSpValue = 1\n",
    "ParchValue = 1\n",
    "\n",
    "PredictionFeatures = (spark.createDataFrame([(SexValue, AgeValue, FareValue, EmbarkedValue, PclassValue, SibSpValue, ParchValue)],\n",
    "    ['Sex', 'Age', 'Fare', 'Embarked', 'Pclass', 'SibSp', 'Parch']))\n",
    "PredictionFeatures.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict whether the imaginary person would have survived\n",
    "### using the best fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SurvivedOrNotPrediction = cvModel.transform(PredictionFeatures)\n",
    "SurvivedOrNotPrediction.select('rawPrediction', 'probability', 'prediction').show(1, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display Prediction Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SurvivedOrNot = SurvivedOrNotPrediction.select(\"prediction\").first()[0]\n",
    "if SurvivedOrNot == 0.0:\n",
    "    print(\"Did NOT Survive\")\n",
    "elif(SurvivedOrNot == 1.0):\n",
    "    print(\"Did Survive!!!\")\n",
    "else:\n",
    "    print(\"Invalid Prediction\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"randomforest\"></a>\n",
    "## 10 - Let's take a quick look at applying the feature engineering performed above to a Random Forest Model\n",
    "### Random forests are ensembles of decision trees. They combine many decision trees in order to reduce the risk of overfitting.\n",
    "### We won't do any hyperparamter tuning in this example, but just show how to create and evaluate the model using all default hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import RandomForestClassificationModel, RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.feature import IndexToString\n",
    "\n",
    "# Index labels, adding metadata to the label column.\n",
    "# Fit on whole dataset to include all labels in index.\n",
    "labelIndexer = StringIndexer().setInputCol(\"Survived\").setOutputCol(\"indexedLabel\").fit(LabeledTitanicData)\n",
    "\n",
    "# Train a RandomForest model\n",
    "rf = RandomForestClassifier().setLabelCol(\"indexedLabel\").setFeaturesCol(\"features\").setNumTrees(20)\n",
    "\n",
    "# Convert indexed labels back to original labels.\n",
    "labelConverter = IndexToString().setInputCol(\"prediction\").setOutputCol(\"predictedLabel\").setLabels(labelIndexer.labels)\n",
    "\n",
    "# Create new Pipeline using the RandomForest model and all the same feature transformers used above for logistic regression\n",
    "pipelineRF = Pipeline().setStages([labelIndexer, SexIndexer, EmbarkedIndexer, AgeBucket, FareBucket, assembler, normalizer, rf, labelConverter])\n",
    "\n",
    "# Train model.\n",
    "modelRF = pipelineRF.fit(train)\n",
    "\n",
    "# Make predictions.\n",
    "predictionsRF = modelRF.transform(test)\n",
    "\n",
    "# Select example rows to display.\n",
    "predictionsRF.select(\"predictedLabel\", \"Survived\", \"features\").show(10)\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "evaluatorRF = MulticlassClassificationEvaluator().setLabelCol(\"Survived\").setPredictionCol(\"prediction\").setMetricName(\"accuracy\")\n",
    "accuracyRF = evaluatorRF.evaluate(predictionsRF)\n",
    "print(\"Accuracy = %g\" % accuracyRF)\n",
    "print(\"Test Error = %g\" % (1.0 - accuracyRF))\n",
    "\n",
    "rfModel = modelRF.stages[7]\n",
    "print(rfModel)  # summary only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"summary\"></a>\n",
    "![IBM Logo](http://www-03.ibm.com/press/img/Large_IBM_Logo_TN.jpg)\n",
    "\n",
    "You created a predictive model that predicts survival probabilities for passengers on the Titanic.\n",
    "\n",
    "  - Load the data\n",
    "  - Cleaned the data\n",
    "  - Created transformers to shape the data\n",
    "  - Created a model using Pipeline\n",
    "  - Split the data into training and test sets\n",
    "  - Tested the model\n",
    "  - Tuned the model\n",
    "  - Tested the model on an imaginary passenger\n",
    "  - Build a second model using Random Forest\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5 (Experimental) with Spark 2.1",
   "language": "python",
   "name": "python3-spark21"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
